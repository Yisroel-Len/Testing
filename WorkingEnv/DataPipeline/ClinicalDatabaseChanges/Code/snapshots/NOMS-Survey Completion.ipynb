{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6799e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:30.503611Z",
     "start_time": "2024-08-29T19:24:30.498545Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc \n",
    "import time\n",
    "import datetime\n",
    "import sqlite3\n",
    "import os\n",
    "from pandas.tseries.offsets import DateOffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af8717",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T18:53:44.694477Z",
     "start_time": "2024-09-17T18:53:44.678617Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "path = \"..\\..\\Logs\\clinical_log.log\"\n",
    "logging.basicConfig(filename=path,\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d,%(name)s,%(levelname)s,%(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.DEBUG)\n",
    "logger = logging.getLogger(\"NOMS-Survey Completion\")\n",
    "# logger.info(\"testing log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456cbdd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DB Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc34cb8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:30.523295Z",
     "start_time": "2024-08-29T19:24:30.519245Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../../InSyncConnection/Database/InSyncClinical.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# # # printing all table names\n",
    "# sql_query = \"\"\"SELECT name FROM sqlite_master\n",
    "#      WHERE type='table';\"\"\"\n",
    "\n",
    "# cursor.execute(sql_query)\n",
    "# print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666fdfd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Encounter Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcff2c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:32.809276Z",
     "start_time": "2024-08-29T19:24:30.525310Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Select all patients from EcounterSummary Table\n",
    "    testPatients = '''\n",
    "    SELECT\n",
    "        PatientId\n",
    "    FROM \n",
    "        emr_PatientDetails\n",
    "    WHERE \n",
    "        LOWER(FirstName) LIKE '%test%'\n",
    "        OR LOWER(LastName) LIKE '%test%'\n",
    "        OR LOWER(FirstName) LIKE '%patient%'\n",
    "        OR LOWER(LastName) LIKE '%patient%'\n",
    "        or CAST(MRNNumber AS INTEGER) < 55\n",
    "    '''\n",
    "    sql=f'''\n",
    "    SELECT \n",
    "        PatientId as PatientID,\n",
    "        VisitDateTime as EncounterDate\n",
    "    FROM \n",
    "        tblEncounterSummary\n",
    "        LEFT JOIN tblENcounterType ON (tblEncounterSummary.EncounterTypeId=tblENcounterType.EncounterTypeId)\n",
    "    WHERE \n",
    "        IsBillable = \"TRUE\"\n",
    "        AND PatientID NOT IN ({testPatients})\n",
    "    ORDER BY \n",
    "        PatientID\n",
    "    '''\n",
    "    encounter_df = pd.read_sql(sql, conn)\n",
    "    encounter_df['EncounterDate'] = pd.to_datetime(encounter_df['EncounterDate'])\n",
    "    \n",
    "    #filter out encounters before 3/1 and keep the first encounter date\n",
    "    encounter_filter = encounter_df['EncounterDate'] >= '2023-03-01'\n",
    "    filtered_dates = encounter_df[encounter_filter].copy()\n",
    "    filtered_dates.sort_values(by=['PatientID', 'EncounterDate'], inplace=True)\n",
    "    encounter_df = filtered_dates.drop_duplicates(subset=\"PatientID\", keep='first').copy()\n",
    "    \n",
    "    # get encounter time by hour\n",
    "    encounter_df['Hour'] = encounter_df[\"EncounterDate\"].dt.floor('h')\n",
    "    \n",
    "    # needed for filtering out transfer patients\n",
    "    encounter_dictionary = dict(zip(encounter_df.PatientID, encounter_df.EncounterDate))\n",
    "    \n",
    "    logger.info(f\"Successfully queried tblEncounterSummary.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to query tblEncounterSummary.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "encounter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b00e2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PatientDetails Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c72803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:32.952391Z",
     "start_time": "2024-08-29T19:24:32.813320Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Select all patients\n",
    "    sql= f'''\n",
    "    SELECT \n",
    "        DISTINCT (PatientID) AS PatientID,\n",
    "        FirstName,\n",
    "        LastName,\n",
    "        DOB,\n",
    "        MRNNumber,\n",
    "        PhoneNo AS Phone\n",
    "    FROM \n",
    "        emr_PatientDetails\n",
    "    WHERE\n",
    "        PatientID NOT IN ({testPatients})\n",
    "    ORDER BY \n",
    "        PatientID\n",
    "    '''\n",
    "    patient_details_df = pd.read_sql(sql, conn)\n",
    "    \n",
    "    # filter out MRN != XXX2\n",
    "    MRN_filter = patient_details_df['MRNNumber'].map(lambda value: True if value[-1] == \"2\" else False)\n",
    "    patient_details_df = patient_details_df[MRN_filter].copy()\n",
    "    \n",
    "    # filter out test patients\n",
    "    test_filter = (patient_details_df['LastName'] != \"Test\") & (patient_details_df[\"FirstName\"] != \"Test\")\n",
    "    patient_details_df = patient_details_df[test_filter].copy()\n",
    "    \n",
    "    logger.info(f\"Successfully queried emr_PatientDetails.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to query emr_PatientDetails.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "\n",
    "patient_details_df[patient_details_df['PatientID'].duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054039c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Discharged Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c9c5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:33.055097Z",
     "start_time": "2024-08-29T19:24:32.957555Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Select all patients\n",
    "    sql = f'''\n",
    "    SELECT \n",
    "        PatientID,\n",
    "        finalEncounterDate AS 'Final Encounter',\n",
    "        is_ActiveInInSync,\n",
    "        is_lastEncounterDischarge\n",
    "    FROM \n",
    "        ptPatient_Activity\n",
    "    WHERE\n",
    "        PatientID NOT IN ({testPatients})\n",
    "    '''\n",
    "    disharge_df = pd.read_sql(sql, conn)\n",
    "    \n",
    "    # filter out active patients\n",
    "    discharged_patients_mask = (disharge_df['is_ActiveInInSync'] == 0) | (disharge_df['is_lastEncounterDischarge'] == 1)\n",
    "    discharged_patients_df = disharge_df[discharged_patients_mask].copy()\n",
    "    \n",
    "    # create discharge status\n",
    "    discharged_patients_df['Status'] = \"Discharged\"\n",
    "    \n",
    "    discharged_patients_df.drop(columns=['is_ActiveInInSync','is_lastEncounterDischarge'],\n",
    "                                axis=1,\n",
    "                                inplace=True)\n",
    "    discharged_patients_df['Final Encounter'] = pd.to_datetime(discharged_patients_df['Final Encounter'])\n",
    "    \n",
    "    logger.info(f\"Successfully queried ptPatient_Activity.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to query ptPatient_Activity.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "discharged_patients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9464e54c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### NOMS Query "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca083be7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Baseline NOMS Taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be402ae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:33.111680Z",
     "start_time": "2024-08-29T19:24:33.058107Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sql=f'''\n",
    "    SELECT \n",
    "        PatientID,\n",
    "        CreatedOn,\n",
    "        StatusDesc AS 'Baseline NOMS'\n",
    "    FROM \n",
    "        tblNOMS_AllAssessments\n",
    "    WHERE \n",
    "        AssessmentKey LIKE '%Baseline%'\n",
    "        AND (StatusDesc LIKE \"%Entered%\"\n",
    "        OR StatusDesc LIKE \"%Completed%\")\n",
    "        AND\n",
    "        PatientID NOT IN ({testPatients})\n",
    "    ORDER BY\n",
    "        PatientID\n",
    "    '''\n",
    "    df = pd.read_sql(sql, conn)\n",
    "    df['CreatedOn'] = pd.to_datetime(df['CreatedOn']).dt.date\n",
    "    \n",
    "    # filter before march\n",
    "    march_mask = datetime.date(2023,3,1)\n",
    "    baseline_df = df[df['CreatedOn'] >= march_mask].copy()\n",
    "    \n",
    "    # Prettify data for mergers later\n",
    "    baseline_df['Baseline NOMS'] = baseline_df['Baseline NOMS'].map(lambda status: True if str(status) in [\"Entered Into SPARS\"] else False)\n",
    "    \n",
    "    logger.info(f\"Successfully queried tblNOMS_AllAssessments for Baselines.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to query tblNOMS_AllAssessments for Baselines.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d150e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Reassessment NOMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403a0457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:33.172864Z",
     "start_time": "2024-08-29T19:24:33.114690Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sql=f'''\n",
    "    SELECT \n",
    "        PatientID,\n",
    "        CreatedOn\n",
    "    FROM \n",
    "        tblNOMS_AllAssessments\n",
    "    WHERE \n",
    "        AssessmentKey LIKE 'Reassessment%'\n",
    "        AND (StatusDesc LIKE \"%Entered%\"\n",
    "        OR StatusDesc LIKE \"%Completed%\")\n",
    "        AND\n",
    "        PatientID NOT IN ({testPatients})\n",
    "    ORDER BY\n",
    "        PatientID\n",
    "    '''\n",
    "    df = pd.read_sql(sql, conn)\n",
    "    df['CreatedOn'] = pd.to_datetime(df['CreatedOn']).dt.date\n",
    "    \n",
    "    # filter before march\n",
    "    march_mask = datetime.date(2023,3,1)\n",
    "    reassessment_df = df[df['CreatedOn'] >= march_mask]\n",
    "    \n",
    "#     # find transfer patients\n",
    "#     transfer_patients=[]\n",
    "#     for patient in reassessment_df['PatientID']:\n",
    "#         if False != encounter_dictionary.get(patient, False):\n",
    "#             transfer_patients.append(patient)\n",
    "            \n",
    "#     # remove transfers if reassessment is before 6 months\n",
    "#     earliest_reassessment_date = datetime.date(2023,8,1)\n",
    "#     for patient in transfer_patients:\n",
    "#         first_encounter = encounter_dictionary.get(patient)\n",
    "#     #     incremented_first_encounter = first_encounter + np.timedelta64(6, 'M')\n",
    "#         reassmessnent_date = df[df['PatientID'] == patient]['CreatedOn']\n",
    "#         print(reassmessnent_date)\n",
    "#         if (reassmessnent_date.values > earliest_reassessment_date) == False:\n",
    "#             reassessment_df = reassessment_df[reassessment_df['PatientID'] != patient].copy()\n",
    "    \n",
    "    # Prettify data for megers later\n",
    "    reassessment_df = reassessment_df[['PatientID']]\n",
    "    reassessment_df['6 Month Reassessment NOMS'] = True\n",
    "    \n",
    "    logger.info(f\"Successfully queried tblNOMS_AllAssessments for Reassessments.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to query tblNOMS_AllAssessments for Reassessments.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a51c1b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Discharge NOMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a987fc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:33.248497Z",
     "start_time": "2024-08-29T19:24:33.178917Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sql=f'''\n",
    "    SELECT \n",
    "        DISTINCT (PatientID),\n",
    "        CreatedOn AS 'Discharge Date',\n",
    "        AssessmentKey AS 'Discharged'\n",
    "    FROM \n",
    "        tblNOMS_AllAssessments\n",
    "    WHERE \n",
    "        Discharged LIKE '%Discharge%'\n",
    "        AND (StatusDesc LIKE \"%Entered%\"\n",
    "        OR StatusDesc LIKE \"%Completed%\")\n",
    "        AND\n",
    "        PatientID NOT IN ({testPatients})\n",
    "    ORDER BY\n",
    "        PatientID\n",
    "    '''\n",
    "    df = pd.read_sql(sql, conn)\n",
    "    df['Discharge Date'] = pd.to_datetime(df['Discharge Date']).dt.date\n",
    "    \n",
    "    # filter before march\n",
    "    march_mask = datetime.date(2023,3,1)\n",
    "    discharge_df = df[df['Discharge Date'] >= march_mask]\n",
    "    \n",
    "    \n",
    "    # Prettify data for megers later\n",
    "    discharge_df = discharge_df[['PatientID']]\n",
    "    discharge_df.drop_duplicates(inplace=True)\n",
    "    discharge_df['Discharge NOMS'] = True\n",
    "\n",
    "    logger.info(f\"Successfully queried tblNOMS_AllAssessments for Discharges.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to query tblNOMS_AllAssessments for Discharges.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "discharge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0cd6f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### NOMS Refusals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1769b8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:33.321593Z",
     "start_time": "2024-08-29T19:24:33.252512Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sql=f'''\n",
    "    SELECT \n",
    "        PatientID,\n",
    "        InterviewConductedNoID AS Reason\n",
    "    FROM \n",
    "        tblNOMS_AllAssessments\n",
    "    WHERE \n",
    "        AssessmentKey LIKE 'Base%'      AND\n",
    "        InterviewConductedID LIKE '%0%' AND\n",
    "        PatientID NOT IN ({testPatients})\n",
    "    '''\n",
    "    refused_df = pd.read_sql(sql, conn)\n",
    "    refused_df['Consent'] = 'No'\n",
    "    # fill in blank reasons with 'None Given'\n",
    "    refused_df = refused_df.replace(r'^\\s*$', 'None Given', regex=True)\n",
    "\n",
    "    # Reorder columns\n",
    "    refused_df = refused_df[['PatientID',\n",
    "                             'Consent',\n",
    "                             'Reason']]\n",
    "    \n",
    "    logger.info(f\"Successfully queried tblNOMS_AllAssessments for Baseline Refusals.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to query tblNOMS_AllAssessments for Baseline Refusals.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "refused_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c6880",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### NOMS Consents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6ccbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:33.392092Z",
     "start_time": "2024-08-29T19:24:33.323611Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sql=f'''\n",
    "    SELECT \n",
    "        PatientID\n",
    "    FROM \n",
    "        tblNOMS_AllAssessments\n",
    "    WHERE \n",
    "        AssessmentKey LIKE 'Base%'      AND\n",
    "        InterviewConductedID LIKE '%1%' AND\n",
    "        PatientID NOT IN ({testPatients})\n",
    "    '''\n",
    "    agreed_df = pd.read_sql(sql, conn)\n",
    "    agreed_df\n",
    "    # add Consent\n",
    "    agreed_df[\"Consent\"] = 'Yes'\n",
    "    \n",
    "    logger.info(f\"Successfully queried tblNOMS_AllAssessments for Baseline Consents.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to query tblNOMS_AllAssessments for Baseline Consents.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "agreed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6b53c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Append Consent dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11066b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:33.403065Z",
     "start_time": "2024-08-29T19:24:33.396109Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "consent_df = pd.concat([refused_df, agreed_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578afc89",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Entered into Spars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617cc29b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065c5aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:29:24.139666Z",
     "start_time": "2024-08-29T19:29:24.070522Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sql=f'''\n",
    "    SELECT \n",
    "        PatientID,\n",
    "        StatusDesc AS 'Baseline SPARS'\n",
    "    FROM \n",
    "        tblNOMSAssessmentDetails\n",
    "    WHERE\n",
    "        PatientID NOT IN ({testPatients})\n",
    "    ORDER BY\n",
    "        PatientID\n",
    "    '''\n",
    "    baseline_SPARS = pd.read_sql(sql,conn)\n",
    "        \n",
    "    logger.info(f\"Successfully queried tblNOMSAssessmentDetails for Baselines entered to SPARS.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to query tblNOMSAssessmentDetails for Baselines entered to SPARS.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c51bf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Reassessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9b928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:29:59.144154Z",
     "start_time": "2024-08-29T19:29:59.082861Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sql=f'''\n",
    "    SELECT \n",
    "        PatientID,\n",
    "        StatusDesc AS 'Reassessment SPARS'\n",
    "    FROM \n",
    "        tblNOMSReAssessmentDetails\n",
    "    WHERE\n",
    "        PatientID NOT IN ({testPatients})\n",
    "    ORDER BY\n",
    "        PatientID\n",
    "    '''\n",
    "    reassessment_SPARS = pd.read_sql(sql,conn)\n",
    "    \n",
    "    logger.info(f\"Successfully queried tblNOMSAssessmentDetails for Reassessments entered to SPARS.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to query tblNOMSAssessmentDetails for Reassessments entered to SPARS.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506b693",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Discharges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8e525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:24:33.556421Z",
     "start_time": "2024-08-29T19:24:33.515450Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sql=f'''\n",
    "    SELECT \n",
    "        PatientID,\n",
    "        StatusDesc AS 'Discharge SPARS'\n",
    "    FROM \n",
    "        tblNOMSDischargeDetails\n",
    "    WHERE\n",
    "        PatientID NOT IN ({testPatients})\n",
    "    ORDER BY\n",
    "        PatientID\n",
    "    '''\n",
    "    discharge_SPARS = pd.read_sql(sql,conn)\n",
    "    discharge_SPARS.drop_duplicates(inplace=True)    \n",
    "    \n",
    "    logger.info(f\"Successfully queried tblNOMSAssessmentDetails for Discharges entered to SPARS.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to query tblNOMSAssessmentDetails for Discharges entered to SPARS.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8331a62",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b050fc1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Merge patient_details and encounters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40229e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:25:49.043946Z",
     "start_time": "2024-08-29T19:25:49.028117Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    full_data_collection_df = patient_details_df.merge(encounter_df,\n",
    "                                                       on='PatientID',\n",
    "                                                       how='left')\n",
    "    # drop patients without encounters\n",
    "    full_data_collection_df = full_data_collection_df[full_data_collection_df['EncounterDate'].notna()]\n",
    "    \n",
    "    logger.info(f\"Successfully merged patient deatils with encounters.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to merge patient deatils with encounters.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b81ef8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Merge in Statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e053746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:25:58.093923Z",
     "start_time": "2024-08-29T19:25:58.079944Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    full_data_collection_df = full_data_collection_df.merge(discharged_patients_df,\n",
    "                                                            on = 'PatientID',\n",
    "                                                            how = 'left')\n",
    "    # fill nan status\n",
    "    full_data_collection_df['Status'].fillna('Active',inplace=True)\n",
    "    \n",
    "    logger.info(f\"Successfully merged in discharged patients.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to merge in discharged patients.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11304ff5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Merge in Baseline NOMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef02019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:26:23.042054Z",
     "start_time": "2024-08-29T19:26:23.026852Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    full_data_collection_df = full_data_collection_df.merge(baseline_df,\n",
    "                                                            on='PatientID',\n",
    "                                                            how='left')\n",
    "    full_data_collection_df['Baseline NOMS'].fillna(False,inplace=True)\n",
    "\n",
    "    logger.info(f\"Successfully merged in Baseline NOMS.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to merge in Baseline NOMS.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b078a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Merge in Reassessment NOMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764bce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:26:38.828568Z",
     "start_time": "2024-08-29T19:26:38.814891Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    full_data_collection_df = full_data_collection_df.merge(reassessment_df,\n",
    "                                                            on='PatientID',\n",
    "                                                            how='left')\n",
    "    full_data_collection_df['6 Month Reassessment NOMS'].fillna(False,inplace=True)\n",
    "\n",
    "    logger.info(f\"Successfully merged in Reassessment NOMS.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to merge in Reassessment NOMS.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519a79f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Merge in Discharge NOMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e639d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:26:46.126252Z",
     "start_time": "2024-08-29T19:26:46.112741Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    full_data_collection_df = full_data_collection_df.merge(discharge_df,\n",
    "                                                            on='PatientID',\n",
    "                                                            how='left')\n",
    "    full_data_collection_df['Discharge NOMS'].fillna(False,inplace=True)\n",
    "\n",
    "    logger.info(f\"Successfully merged in Discharge NOMS.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to merge in Discharge NOMS.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f88bc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Merge in SPARS Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7842c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:27:10.324699Z",
     "start_time": "2024-08-29T19:27:10.302836Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try: # add the baseline SPARS\n",
    "    full_data_collection_df = full_data_collection_df.merge(baseline_SPARS,\n",
    "                                                            on='PatientID',\n",
    "                                                            how='left')\n",
    "    logger.info(f\"Successfully merged in Baseline SPARS.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to merge in Baseline SPARS.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "\n",
    "try: # add the reassessment SPARS\n",
    "    full_data_collection_df = full_data_collection_df.merge(reassessment_SPARS,\n",
    "                                                        on='PatientID',\n",
    "                                                        how='left')\n",
    "    logger.info(f\"Successfully merged in Reassessment SPARS.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to merge in Reassessment SPARS.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "try: # add the discharge SPARS\n",
    "    full_data_collection_df = full_data_collection_df.merge(discharge_SPARS,\n",
    "                                                        on='PatientID',\n",
    "                                                        how='left')\n",
    "    logger.info(f\"Successfully merged in Discharge SPARS.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to merge in Discharge SPARS.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "\n",
    "foo = full_data_collection_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6618acd4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Merge in New Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855e5a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:21:52.743390Z",
     "start_time": "2024-08-29T19:21:52.719927Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NoteSubTypeDict = {5: \"First Attempt\",\n",
    "                    6: 'First Attempt',\n",
    "                    7: \"First Attempt\"}\n",
    "NoteSubTypeColumnDict = {5: 'Baseline SPARS',\n",
    "                          6: \"Reassessment SPARS\",\n",
    "                          7: \"Discharge SPARS\"}\n",
    "try:\n",
    "    sql=f'''\n",
    "    SELECT \n",
    "        *\n",
    "    FROM \n",
    "        tblPatientNotes\n",
    "    WHERE\n",
    "        NoteSubType IN (5, 6, 7)\n",
    "    ORDER BY\n",
    "        PatientID\n",
    "    '''\n",
    "    patientNotes = pd.read_sql(sql,conn)\n",
    "    \n",
    "    patientNotes['NoteAddedOn'] = pd.to_datetime(patientNotes['NoteAddedOn'])\n",
    "    patientNotes.sort_values(by='NoteAddedOn', ascending=False)\n",
    "    patientNotes['NoteSubTypeName'] = patientNotes['NoteSubType'].map(lambda subtypeID: NoteSubTypeDict[int(subtypeID)])\n",
    "    patientNotes['NoteSubTypeColumn'] = patientNotes['NoteSubType'].map(lambda subtypeID: NoteSubTypeColumnDict[int(subtypeID)])\n",
    "    patientNotesPivot = patientNotes.pivot(index=['PatientID'], columns = \"NoteSubTypeColumn\", values=\"NoteSubTypeName\").reset_index()\n",
    "    patientbaselineDict = patientNotesPivot.set_index('PatientID').to_dict()['Baseline SPARS']\n",
    "    patientReassessmentDict = patientNotesPivot.set_index('PatientID').to_dict()['Reassessment SPARS']\n",
    "    patientDischargeDict = patientNotesPivot.set_index('PatientID').to_dict()['Discharge SPARS']\n",
    "\n",
    "    def checkforBaselineNotes(patientID, note):\n",
    "        if note == \"\" or str(note) == 'nan':\n",
    "            return patientbaselineDict.get(patientID, \"\")\n",
    "        else:\n",
    "            return note\n",
    "\n",
    "    def checkforReassessmentNotes(patientID, note):\n",
    "        if note == \"\" or str(note) == 'nan':\n",
    "            return patientReassessmentDict.get(patientID, \"\")\n",
    "        else:\n",
    "            return note\n",
    "\n",
    "    def checkforDischargeNotes(patientID, note):\n",
    "        if note == \"\" or str(note) == 'nan':\n",
    "            return patientDischargeDict.get(patientID, \"\")\n",
    "        else:\n",
    "            return note\n",
    "    full_data_collection_df['Baseline SPARS'] = full_data_collection_df.apply(lambda row: checkforBaselineNotes(row['PatientID'], row['Baseline SPARS']), axis=1)\n",
    "    full_data_collection_df['Reassessment SPARS'] = full_data_collection_df.apply(lambda row: checkforReassessmentNotes(row['PatientID'], row['Reassessment SPARS']), axis=1)\n",
    "    full_data_collection_df['Discharge SPARS'] = full_data_collection_df.apply(lambda row: checkforDischargeNotes(row['PatientID'], row['Discharge SPARS']), axis=1)\n",
    "#     logger.info(f\"Successfully queried tblNotes and created columns.\")\n",
    "except Exception as e:\n",
    "#     logger.error(f\"Failed to query table notes and creat columns.\", exc_info=True) \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf83cf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prettify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8a108",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:19:30.788985Z",
     "start_time": "2024-08-29T19:19:30.518868Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_data_collection_df = foo.copy()\n",
    "\n",
    "try: # Format Names\n",
    "    first_name = full_data_collection_df['FirstName'].map(lambda name: str(name).capitalize())\n",
    "    last_name = full_data_collection_df['LastName'].map(lambda name: str(name).capitalize())\n",
    "    full_data_collection_df['Name'] = last_name + ', ' + first_name\n",
    "    full_data_collection_df.drop(columns=['FirstName','LastName'], inplace=True)\n",
    "    \n",
    "    logger.info(f\"Successfully formated names.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to format names.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "try: # Baseline Date\n",
    "    full_data_collection_df['Hour'] = pd.to_datetime(full_data_collection_df['Hour'])\n",
    "    full_data_collection_df['Baseline Due Date'] = full_data_collection_df['Hour'] + DateOffset(days=30)\n",
    "    full_data_collection_df['Baseline Due Date'] = full_data_collection_df['Baseline Due Date'].map(lambda num: num.strftime('%m-%d-%Y'))\n",
    "\n",
    "    logger.info(f\"Successfully added Baseline Due Date.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to add Baseline Due Date.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "try:    # Baseline Warning\n",
    "    full_data_collection_df['Baseline Due Date'] = pd.to_datetime(full_data_collection_df['Baseline Due Date'])\n",
    "    full_data_collection_df['Baseline Warning'] = (full_data_collection_df['Baseline Due Date'] <= (pd.Timestamp.today())) & (full_data_collection_df['Baseline NOMS'] == False)\n",
    "    \n",
    "    logger.info(f\"Successfully added Baseline Warning.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to add Baseline Warning.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "try:    # 6 Month Reassessment Date\n",
    "    # we're going to set the default encounter date to the 15th of the month to match SPARS\n",
    "    full_data_collection_df['Hour'] = full_data_collection_df['Hour'].apply(lambda dt: dt.replace(day=15))\n",
    "    full_data_collection_df['6 Month Reassessment Date'] = full_data_collection_df['Hour'] + DateOffset(months=6)\n",
    "    full_data_collection_df['6 Month Reassessment Date'] = full_data_collection_df['6 Month Reassessment Date'].map(lambda num: num.strftime('%m-%d-%Y'))\n",
    "\n",
    "    logger.info(f\"Successfully added Reassessment Date.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to add Reassessment Date.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "try:    # 6 Month Warning\n",
    "    full_data_collection_df['6 Month Reassessment Date'] = pd.to_datetime(full_data_collection_df['6 Month Reassessment Date'])\n",
    "    full_data_collection_df['6 Month Reassessment Warning'] = full_data_collection_df['6 Month Reassessment Date'] - DateOffset(months=1) <= pd.Timestamp.today()\n",
    "\n",
    "    logger.info(f\"Successfully added Reassessment Warning.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to add Reassessment Warning.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "try:    # add days since first encounter\n",
    "    today = datetime.date.today()\n",
    "    today = pd.to_datetime(today)\n",
    "    def daysSinceFirstEncounter(row):\n",
    "        date = row['EncounterDate']\n",
    "        if type(date) == pd.Timestamp:\n",
    "            return (today - date).days\n",
    "        return np.nan\n",
    "#     def daysTillBLineCompletion(row):\n",
    "#         encounter_date = row['EncounterDate']\n",
    "#         if isinstance(encounter_date, pd.Timestamp):\n",
    "            \n",
    "    def daysTillThirtyDaysAfter(row):\n",
    "        encounter_date = row['EncounterDate']\n",
    "        if isinstance(encounter_date, pd.Timestamp):\n",
    "            # Calculate thirty days after the encounter date\n",
    "            thirty_days_after = encounter_date + pd.DateOffset(days=30)\n",
    "            days_till_thirty_days_after = (thirty_days_after - pd.Timestamp.today()).days\n",
    "            return days_till_thirty_days_after\n",
    "        return np.nan\n",
    "    def daysTillSevenMonths(row):\n",
    "        encounter_date = row['EncounterDate']\n",
    "        if isinstance(encounter_date, pd.Timestamp):\n",
    "            # Calculate seven months from the encounter date\n",
    "            seven_months_later = encounter_date + pd.DateOffset(months=7)\n",
    "            # Calculate the number of days between today and seven months later\n",
    "            days_till_seven_months = (seven_months_later - pd.Timestamp.today()).days\n",
    "            return days_till_seven_months\n",
    "        return np.nan\n",
    "    full_data_collection_df['Days Since Encounter'] = full_data_collection_df.apply(lambda row: daysSinceFirstEncounter(row), axis=1)  \n",
    "    full_data_collection_df['Days Till Reassessment Date'] = full_data_collection_df.apply(lambda row: daysTillSevenMonths(row), axis=1)  \n",
    "    full_data_collection_df['Days Till Baseline Date'] = full_data_collection_df.apply(lambda row: daysTillThirtyDaysAfter(row), axis=1)  \n",
    "    logger.info(f\"Successfully added Days Since First Encounter and days remaining till baseline/reassessment due dates.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to add Days Since First Encounter and days remaining till baseline/reassessment due dates.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "\n",
    "try:    # add days since final encounter\n",
    "    def daysSinceDischarge(row):\n",
    "        date = row['Final Encounter']\n",
    "        if type(date) == pd.Timestamp:\n",
    "            return (today - date).days\n",
    "        return np.nan\n",
    "    full_data_collection_df['Days Since Final Encounter'] = full_data_collection_df.apply(lambda row: daysSinceDischarge(row), axis=1)\n",
    "\n",
    "    logger.info(f\"Successfully added Days Since Final Encounter.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to add Days Since Final Encounter.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "try:     # find all dischareged patients and reset Status to \"Discharged\" \n",
    "    discharge_mask_1 = full_data_collection_df['Status'] == \"Discharged\"\n",
    "    discharge_mask_2 = full_data_collection_df['Discharge NOMS'] == True\n",
    "    discharge_mask_3 = full_data_collection_df['Discharge SPARS'] == 'Entered Into SPARS'\n",
    "    completed_discharge_mask = np.logical_or(discharge_mask_1, discharge_mask_2, discharge_mask_3)\n",
    "    full_data_collection_df.loc[completed_discharge_mask, 'Status'] = 'Discharged'\n",
    "\n",
    "    logger.info(f\"Successfully updated discharged statuses.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to update discharged statuses.\", exc_info=True) \n",
    "    print(e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdde660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:19:30.830346Z",
     "start_time": "2024-08-29T19:19:30.792089Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_data_collection_df[full_data_collection_df['PatientID'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda910fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T19:19:30.839979Z",
     "start_time": "2024-08-29T19:19:30.832355Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reorder Columns\n",
    "full_data_collection_df = full_data_collection_df[['PatientID',\n",
    "                                                   'Name',\n",
    "                                                   'DOB',\n",
    "                                                   'Phone',\n",
    "                                                   'MRNNumber',\n",
    "                                                   'Status',\n",
    "                                                   'EncounterDate',\n",
    "                                                   'Hour',\n",
    "                                                   'Days Since Encounter',\n",
    "                                                   'Days Till Reassessment Date',\n",
    "                                                   'Days Till Baseline Date',\n",
    "                                                   'Baseline NOMS',\n",
    "                                                   'Baseline Due Date',\n",
    "                                                   'Baseline Warning',\n",
    "                                                   'Baseline SPARS',\n",
    "                                                   '6 Month Reassessment NOMS',\n",
    "                                                   '6 Month Reassessment Date',\n",
    "                                                   '6 Month Reassessment Warning',\n",
    "                                                   'Reassessment SPARS',\n",
    "                                                   'Final Encounter',\n",
    "                                                   'Days Since Final Encounter',\n",
    "                                                   'Discharge NOMS',\n",
    "                                                   'Discharge SPARS']]\n",
    "full_data_collection_df.drop_duplicates('PatientID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ce300",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Insync Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97030aa3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# looking for anomalies, NOMS == False but SPARS == True\n",
    "try:    # Baseline anomalies\n",
    "    baseline_SPARS = full_data_collection_df['Baseline SPARS'] == \"Entered Into SPARS\"\n",
    "    baseline_NOMS = full_data_collection_df['Baseline NOMS'] == True\n",
    "    Insync_baseline_anomalies = ~baseline_NOMS & baseline_SPARS\n",
    "    Insync_baseline_anomalies = full_data_collection_df[Insync_baseline_anomalies]\n",
    "    Insync_baseline_anomalies.insert(2,'Anomaly Description', \"Baseline: Has SPARS, missing NOMS\")\n",
    "\n",
    "    logger.info(f\"Successfully checked for baseline Anomalies.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to check for baseline Anomalies.\", exc_info=True) \n",
    "    print(e)\n",
    "    \n",
    "try:        # reassessment anomalies\n",
    "    reassessment_SPARS = full_data_collection_df['Reassessment SPARS'] == \"Entered Into SPARS\"\n",
    "    reassessment_NOMS = full_data_collection_df['6 Month Reassessment NOMS'] == True\n",
    "    Insync_reassessment_anomalies = ~reassessment_NOMS & reassessment_SPARS\n",
    "    Insync_reassessment_anomalies = full_data_collection_df[Insync_reassessment_anomalies]\n",
    "    Insync_reassessment_anomalies.insert(2,'Anomaly Description', \"Reassessment: Has SPARS, missing NOMS\")\n",
    "\n",
    "    logger.info(f\"Successfully checked for reassessment Anomalies.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to check for reassessment Anomalies.\", exc_info=True) \n",
    "    print(e)\n",
    "    \n",
    "try:        # discharge anomalies\n",
    "    discharge_SPARS = full_data_collection_df['Discharge SPARS'] == \"Entered Into SPARS\"\n",
    "    discharge_NOMS = full_data_collection_df['Discharge NOMS'] == True\n",
    "    Insync_discharge_anomalies = ~discharge_NOMS & discharge_SPARS\n",
    "    Insync_discharge_anomalies = full_data_collection_df[Insync_discharge_anomalies]\n",
    "    Insync_discharge_anomalies.insert(2,'Anomaly Description', \"Discharge: Has SPARS, missing NOMS\")\n",
    "\n",
    "    logger.info(f\"Successfully checked for discharge Anomalies.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to check for discharge Anomalies.\", exc_info=True) \n",
    "    print(e)\n",
    "    \n",
    "try:        # concat anomalies into their own df\n",
    "    anomaly_info = ['PatientID','Anomaly Description']\n",
    "    Insync_anomalies = pd.concat([Insync_baseline_anomalies[anomaly_info],\n",
    "                                  Insync_reassessment_anomalies[anomaly_info],\n",
    "                                  Insync_discharge_anomalies[anomaly_info]])\n",
    "    \n",
    "    logger.info(f\"Successfully created insync anomalies df.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create insync anomalies df.\", exc_info=True) \n",
    "    print(e)\n",
    "    \n",
    "Insync_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0a794",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get Data from SPARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8dbf8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:     #read in SPARS excel\n",
    "    SPARS_data_path = \"..\\Data\\SPARS Download 6-24-2024.xlsx\" # does it download with date in name? will break if yes\n",
    "    SPARS_data_df = pd.read_excel(SPARS_data_path)\n",
    "\n",
    "    logger.info(f\"Successfully read in SPARS excel.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to read in SPARS excel.\", exc_info=True) \n",
    "    print(e)\n",
    "# SPARS_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e08f9d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Filter down SPARS to match Insync "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d5fec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:     # filter down MRN numbers\n",
    "    SPARS_data_df['ClientID'] = SPARS_data_df['ClientID'].str.replace(\"'\",\"\")\n",
    "    MRN_mask  = SPARS_data_df['ClientID'].map(lambda value: True if value[-1] == \"2\" else False)\n",
    "    SPARS_data_df = SPARS_data_df[MRN_mask]\n",
    "\n",
    "    logger.info(f\"Successfully filtered SPARS MRNs.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to filter SPARS MRNs.\", exc_info=True) \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492a05e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:    # filter first encounters after march\n",
    "    march_mask = SPARS_data_df['FirstReceivedServicesDate'] > '2023-03-01'\n",
    "    SPARS_data_df = SPARS_data_df[march_mask]\n",
    "\n",
    "    logger.info(f\"Successfully filtered SPARS first encounters.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to filter SPARS first encounters.\", exc_info=True) \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad67c3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:    # filter out other programs\n",
    "    program_mask = SPARS_data_df['ClientID'].map(lambda value: True if value[0] == \"0\" else False)\n",
    "    SPARS_data_df = SPARS_data_df[program_mask]\n",
    "    \n",
    "    logger.info(f\"Successfully filtered SPARS other programs.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to filter SPARS other programs.\", exc_info=True) \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf34ea0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa904f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_wanted = ['ClientID','Assessment','CalculatedInterviewDate','ConductedInterview','WhyNotConducted']\n",
    "assessment_dictionary = {\n",
    "    \"Baseline\" : 600,\n",
    "    \"Reassessment\": 601,\n",
    "    \"Discharge\" : 699\n",
    "}\n",
    "interview_not_conducted_dictionary = {\n",
    "    1 : \"Not able to obtain consent from proxy\",\n",
    "    2 : \"Client was impaired or unable to provide consent\",\n",
    "    3 : \"Client refused this interview\",\n",
    "    4 : \"Client was not reached for interview\",\n",
    "    5 : \"Client refused all interviews\",\n",
    "    -1 : \"Client was interviewed\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f1076",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    SPARS_assessments = pd.DataFrame(columns = columns_wanted) # create df for the desired data \n",
    "    for assessment in assessment_dictionary:\n",
    "        assessment_df = SPARS_data_df[SPARS_data_df['Assessment'] == assessment_dictionary.get(assessment)] # copy in all data for current assessment type\n",
    "        assessment_df = assessment_df.drop_duplicates(['ClientID']) # get rid of duplicates on name (shouldn't be needed but there was a duplicated baseline)\n",
    "        assessment_df = assessment_df[columns_wanted] # remove all unneeded columns \n",
    "        assessment_df['Assessment'] = assessment # set assessment type values\n",
    "        SPARS_assessments = pd.concat([SPARS_assessments[columns_wanted], # add current assessment data to all assessments' data\n",
    "                                       assessment_df[columns_wanted]])\n",
    "    SPARS_assessments.rename(columns={\"ClientID\": \"MRN Number\"},inplace=True)\n",
    "    SPARS_assessments\n",
    "    \n",
    "    logger.info(f\"Successfully selected useful SPARS data.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to select useful SPARS data.\", exc_info=True) \n",
    "    print(e)\n",
    "SPARS_assessments    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40921e93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Replace Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32badf06",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:    # replace values using dictionaries from SPARS\n",
    "    for reason in interview_not_conducted_dictionary:\n",
    "        SPARS_assessments['WhyNotConducted'] = SPARS_assessments['WhyNotConducted'].replace(reason, interview_not_conducted_dictionary.get(reason))\n",
    "\n",
    "    logger.info(f\"Successfully replaced values for SPARS not conducted.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to replace values for SPARS not conducted.\", exc_info=True) \n",
    "    print(e)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590ea7a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compare with Insync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b59d36",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:    # which MRNs exist only Insync or SPARS but not both\n",
    "    full_data_collection_df['SPARS MRN'] = full_data_collection_df['MRNNumber'].isin(SPARS_assessments['MRN Number'].drop_duplicates())\n",
    "    SPARS_assessments['Insync MRN'] = SPARS_assessments['MRN Number'].isin(full_data_collection_df['MRNNumber'].drop_duplicates())\n",
    "    \n",
    "    logger.info(f\"Successfully compared SPARS and Insync MRNs.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to compare SPARS and Insync MRNs.\", exc_info=True) \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c0cb6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get download Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144e417",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:    # add download date\n",
    "    file_stat = os.stat(SPARS_data_path)\n",
    "\n",
    "    # Check if st_birthtime attribute is available\n",
    "    if hasattr(file_stat, 'st_birthtime'):\n",
    "        # Use st_birthtime for the creation time\n",
    "        creation_time = file_stat.st_birthtime\n",
    "    else:\n",
    "        # Fallback to st_mtime for the last modification time\n",
    "        creation_time = file_stat.st_mtime\n",
    "    \n",
    "    # Convert the creation time to a datetime object\n",
    "    creation_datetime = datetime.datetime.fromtimestamp(creation_time)\n",
    "    # creation_datetime = datetime.fromtimestamp(creation_time)\n",
    "    \n",
    "    # Format the datetime object into MM/DD/YYYY\n",
    "    creation_date_formatted = creation_datetime.strftime('%m/%d/%Y')\n",
    "    \n",
    "    SPARS_assessments['Download Date'] = creation_date_formatted\n",
    "\n",
    "    logger.info(f\"Successfully added SPARS download date.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to add SPARS download date.\", exc_info=True) \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b262565",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Push to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde36c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T14:57:03.137364Z",
     "start_time": "2023-08-08T14:57:03.134501Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with pd.ExcelWriter(r\"../data/NOMS Survey Completion.xlsx\") as writer: \n",
    "#     full_data_collection_df.to_excel(writer, sheet_name=\"Patient Data\",index = False)\n",
    "#     consent_df.to_excel(writer, sheet_name=\"Consent Data\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4d841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T14:57:03.141960Z",
     "start_time": "2023-08-08T14:57:03.138365Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \", \".join([item + \" \" + str(full_data_collection_df[item].dtype) for item in full_data_collection_df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29182280",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Push full_data_collection (Insync data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfcddf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T14:57:03.181493Z",
     "start_time": "2023-08-08T14:57:03.143958Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_name = \"ptNOMS_Completion\"\n",
    "try:\n",
    "    c = conn.cursor()\n",
    "    c.execute(f'''CREATE TABLE IF NOT EXISTS {table_name} (PatientID INTEGER,\n",
    "                                                           Name TEXT,\n",
    "                                                           DOB TEXT,\n",
    "                                                           Phone, TEXT\n",
    "                                                           MRNNumber TEXT,\n",
    "                                                           Status TEXT,\n",
    "                                                           EncounterDate TEXT,\n",
    "                                                           Hour TEXT,\n",
    "                                                           DaysSinceEncounter INTEGER,\n",
    "                                                           DaysTillBaseline INTEGER,\n",
    "                                                           DaysTillReassessment INTEGER\n",
    "                                                           BaselineNOMS TEXT,\n",
    "                                                           BaselineDueDate TEXT,\n",
    "                                                           BaselineWarning TEXT,\n",
    "                                                           Baseline SPARS TEXT,\n",
    "                                                           _6MonthReassessmentNOMS TEXT,\n",
    "                                                           _6MonthReassessmentDate TEXT,\n",
    "                                                           _6MonthReassessmentWarning TEXT,\n",
    "                                                           ReassessmentSPARS TEXT,\n",
    "                                                           FinalEncounter TEXT,\n",
    "                                                           DaysSinceFinalEncounter INTEGER,\n",
    "                                                           DischargeNOMS TEXT,\n",
    "                                                           DischargeSPARS TEXT,\n",
    "                                                           SPARS MRN TEXT)''')\n",
    "    conn.commit()\n",
    "    logger.info(f\"Successfully created {table_name}.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create {table_name}.\", exc_info=True) \n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    full_data_collection_df.to_sql(table_name, conn, if_exists='replace', index = False)\n",
    "    logger.info(f\"Successfully pushed data to {table_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to push data to {table_name}.\", exc_info=True) \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2321acf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Push consent_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc32ab6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T14:57:03.186393Z",
     "start_time": "2023-08-08T14:57:03.182494Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \", \".join([item + \" \" + str(consent_df[item].dtype) for item in consent_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95be822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T14:57:03.211904Z",
     "start_time": "2023-08-08T14:57:03.189388Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_name = \"ptNOMS_Consent\"\n",
    "try:\n",
    "    c = conn.cursor()\n",
    "    c.execute(f'''CREATE TABLE IF NOT EXISTS {table_name} (PatientID INTEGER,\n",
    "                                                           Consent TEXT,\n",
    "                                                           Reason TEXT)''')\n",
    "    conn.commit()\n",
    "    logger.info(f\"Successfully created {table_name}.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create {table_name}.\", exc_info=True) \n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    consent_df.to_sql(table_name, conn, if_exists='replace', index = False)\n",
    "    logger.info(f\"Successfully pushed data to {table_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to push data to {table_name}.\", exc_info=True) \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7c1a2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Push Insync Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdaff7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_name = \"ptNOMS_Insync_Anomalies\"\n",
    "\n",
    "try:\n",
    "    c = conn.cursor()\n",
    "    c.execute(f'''CREATE TABLE IF NOT EXISTS {table_name} (Patient ID INTEGER,\n",
    "                                                           Anomaly Description TEXT)''')\n",
    "    conn.commit()\n",
    "    logger.info(f\"Successfully created {table_name}.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create {table_name}.\", exc_info=True) \n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    Insync_anomalies.to_sql(table_name, conn, if_exists='replace', index = False)\n",
    "    logger.info(f\"Successfully pushed data to {table_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to push data to {table_name}.\", exc_info=True) \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431ca3c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Push SPARS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59123a3-5592-425b-80c5-43e1a1794d7e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SPARS_assessments.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624a1ac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_name = \"ptNOMS_SPARS_Data\"\n",
    "\n",
    "try:\n",
    "    c = conn.cursor()\n",
    "    c.execute(f'''CREATE TABLE IF NOT EXISTS {table_name} (MRN Number TEXT,\n",
    "                                                           Assessment Type TEXT,\n",
    "                                                           Interview Conducted INTEGER,\n",
    "                                                           WhyNotConducted TEXT,\n",
    "                                                           Insync MRN TEXT,\n",
    "                                                           Download Date TEXT)''')\n",
    "    conn.commit()\n",
    "    logger.info(f\"Successfully created {table_name}.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create {table_name}.\", exc_info=True) \n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    SPARS_assessments.to_sql(table_name, conn, if_exists='replace', index = False)\n",
    "    logger.info(f\"Successfully pushed data to {table_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to push data to {table_name}.\", exc_info=True) \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b333663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T14:57:03.216874Z",
     "start_time": "2023-08-08T14:57:03.212905Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c9e3d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full_data_collection_df.to_clipboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 0.116952,
   "end_time": "2024-10-15T15:39:23.667293",
   "environment_variables": {},
   "exception": null,
   "input_path": "NOMS-Survey Completion.ipynb",
   "output_path": "snapshots/NOMS-Survey Completion.ipynb",
   "parameters": {},
   "start_time": "2024-10-15T15:39:23.550341",
   "version": "2.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "67px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "499.56px",
    "left": "35.9744px",
    "right": "20px",
    "top": "144.972px",
    "width": "471.449px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}